{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIC_Autoencoder_DNNTf2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwBoUy3pwOqu"
      },
      "source": [
        "### Importación de Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WObO5QzEv7sw"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvlHBqDKwTyR"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB-4LYkAwYUM"
      },
      "source": [
        "### Importación de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UpIfijvwXej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd80f28-d16f-4506-eba8-991bf377b7fe"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data( )"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EuFzrkwwe32"
      },
      "source": [
        "### verificando los datos de prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS-ByVk5wWkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "453cf29b-24c9-4ce9-d48d-b71c351d184d"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiRgIMtnwjxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "640e0f65-65a4-4f26-c864-977163dd3bfa"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbrhYWFOwlo4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "06b48fa0-87ea-4b2f-8056-16066601282c"
      },
      "source": [
        "imagendemo=x_train[24]\n",
        "plt.imshow(imagendemo,cmap='gray')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f52ee3cd080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lch6FBqwngl"
      },
      "source": [
        "def image_matrix(img):\n",
        "    print('\\n'.join([''.join(['{:4}'.format(int(round(item*255))) for item in row]) \n",
        "      for row in img]))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV-UqIWSwqOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2800111d-b834-44e6-acc9-0759b991b1d3"
      },
      "source": [
        "image_matrix(imagendemo)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0423305661014025   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   05023564770555901275   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   073956349564770647702295   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   011475647706477044370 510   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0102041820647706477021675   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   03723064770647706477021675   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0257556247564770647706477021675   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   024735632406477052020647706477021675   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   03060150452499038505604356477064770277958925647706477021675   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0104555508064770647706094539015943510208160647706477021675   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0178511220112207650   0   0   08160647706477024480   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   04845586506477044370   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0502356477028050   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0502356477021675   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0502356451516065   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   094351377013770114756630214205635521420535579054131019890   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   015301045535955622206477064770632406018064770647706477059415609456477035190   0   0   0   0   0   0\n",
            "   0   0   0   0   0   05865425856477064770647706477058395581404717535190351903519035190351903519011220   0   0   0   0   0   0\n",
            "   0   0   0   0   0   02881564770647706477045645163201275   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   08160532954666524735   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA46NuqYwuwr"
      },
      "source": [
        "## Preparando los datos para el entrenamiento\n",
        " \n",
        "La x debe ser convertida a un vector para que pueda ser procesada por la red perceptrón profunda "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMHwh152wr0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8517f92d-04c9-46b9-b44e-0e266ef509c2"
      },
      "source": [
        "x_train=x_train.reshape(-1,28*28).astype('float32')\n",
        "x_test=x_test.reshape(-1,28*28).astype('float32')\n",
        "x_train.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxlXFkFkwzx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71eab3ad-24d9-4670-d271-cedc2c1014d4"
      },
      "source": [
        "x_train"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1bqZLZMw56I"
      },
      "source": [
        "Las salidas y deben se codificadas en one hot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt9NRYJAw1Vp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7b6710-a216-4729-cbaa-7e5af4188722"
      },
      "source": [
        "# onehot encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "y_train = y_train.reshape(len(y_train), 1)\n",
        "y_train_onehot = onehot_encoder.fit_transform(y_train)\n",
        "\n",
        "y_test = y_test.reshape(len(y_test), 1)\n",
        "y_test_onehot = onehot_encoder.fit_transform(y_test)\n",
        "\n",
        "y_train_onehot.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydARdOP_xBT8"
      },
      "source": [
        "### Declarando la arquitectura\n",
        "\n",
        "Generando función "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyLEoZW-w3kD"
      },
      "source": [
        "#EN TÉRMINOS GENERALES: Los autoencoder deben de tener arquitectura espejo, para realizar correctamente la decodificación. 784-50-2-50-784\n",
        "class DNN_model(object):\n",
        "  def __init__(self,\n",
        "               n_nodes_hl1=50, #primera capa con 50 pixeles. La imagen original tiene 784\n",
        "               n_nodes_hl2=10, #segunda capa con 2 pixeles, está comprobado que funciona con 2 para reconstruir. Con 10-15 funciona mejor\n",
        "               n_nodes_hl3=50,\n",
        "               n_classes=784):\n",
        "    self.h1LW = tf.Variable(np.random.normal(size=(784, n_nodes_hl1)),name=\"hl1weigths\",dtype=\"float32\")\n",
        "    self.h1LB = tf.Variable(np.random.normal(size=(n_nodes_hl1)),name=\"hl1bias\",dtype=\"float32\")\n",
        "    self.h2LW = tf.Variable(np.random.normal(size=(n_nodes_hl1, n_nodes_hl2)),name=\"hl2weigths\",dtype=\"float32\")\n",
        "    self.h2LB = tf.Variable(np.random.normal(size=(n_nodes_hl2)),name=\"hl2bias\",dtype=\"float32\")\n",
        "    self.h3LW = tf.Variable(np.random.normal(size=(n_nodes_hl2, n_nodes_hl3)),name=\"hl3weigths\",dtype=\"float32\")\n",
        "    self.h3LB = tf.Variable(np.random.normal(size=(n_nodes_hl3)),name=\"hl3bias\",dtype=\"float32\")\n",
        "    self.outW = tf.Variable(np.random.normal(size=(n_nodes_hl3, n_classes)),name=\"outweigths\",dtype=\"float32\")\n",
        "    self.outB = tf.Variable(np.random.normal(size=(n_classes)),name=\"outbias\",dtype=\"float32\")\n",
        "    self.trainable_variables =[self.h1LW,self.h1LB,self.h2LW,self.h2LB,self.h3LW,self.h3LB,self.outW,self.outB]          \n",
        "  def __call__(self,x): \n",
        "      # Declarando la arquitectura\n",
        "\n",
        "      l1 = tf.add(tf.matmul(x,self.h1LW), self.h1LB)\n",
        "      l1 = tf.nn.tanh(l1)\n",
        "\n",
        "      l2 = tf.add(tf.matmul(l1,self.h2LW), self.h2LB)\n",
        "      l2 = tf.nn.tanh(l2)\n",
        "\n",
        "      l3 = tf.add(tf.matmul(l2,self.h3LW), self.h3LB)\n",
        "      l3 = tf.nn.tanh(l3)\n",
        "\n",
        "      l4 = tf.add(tf.matmul(l3,self.outW), self.outB)\n",
        "      output = tf.nn.relu(l4)\n",
        "      return output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB_ocuTDRpm2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a28195-e841-45b9-e6af-6de095675872"
      },
      "source": [
        "DNN = DNN_model()\n",
        "DNN(x_train[24:30])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6, 784), dtype=float32, numpy=\n",
              "array([[0.       , 1.9513354, 3.5381095, ..., 0.3272009, 0.3951751,\n",
              "        0.       ],\n",
              "       [2.4953666, 8.063675 , 0.       , ..., 0.       , 0.       ,\n",
              "        0.       ],\n",
              "       [6.6892858, 0.       , 3.6158996, ..., 0.       , 0.       ,\n",
              "        0.       ],\n",
              "       [0.       , 6.399375 , 0.       , ..., 0.       , 0.       ,\n",
              "        0.       ],\n",
              "       [1.3137885, 0.       , 0.       , ..., 5.9124794, 0.       ,\n",
              "        0.       ],\n",
              "       [0.       , 8.484781 , 3.1787727, ..., 6.5098877, 2.4745061,\n",
              "        0.       ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDrsTi04x6p4"
      },
      "source": [
        "Seleccionar un optimizador "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1skjGUXfx1v3"
      },
      "source": [
        "#optimizador = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-1)\n",
        "optimizador = tf.keras.optimizers.Adam(learning_rate=.0001 )"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xadZVhbyAvW"
      },
      "source": [
        "### Definir las metricas a usar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yltF7JCx_5i"
      },
      "source": [
        "#La métrica cambia a regresión\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "506fghB5yQt1"
      },
      "source": [
        "### Calculo de gradientes y ajuste "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5y-qGYTyN7V"
      },
      "source": [
        "#En este caso las etiquetas son iguales a la entrada\n",
        "@tf.function\n",
        "def train_step(model,tdata, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(tdata)\n",
        "    #calculo de una funcion de error \n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(labels, predictions))\n",
        "   \n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  capped_grads_and_vars = [(grad,model.trainable_variables[index]) for index, grad in enumerate(gradients)]\n",
        "  optimizador.apply_gradients(capped_grads_and_vars)\n",
        "  train_loss(loss)\n",
        "  "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DohX8vI-yZd9"
      },
      "source": [
        "@tf.function\n",
        "def test_step(model,tdata, labels):\n",
        "  predictions = model(tdata)\n",
        "  t_loss =  tf.reduce_mean(tf.math.squared_difference(labels, predictions))\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbl1Fogmyezy"
      },
      "source": [
        "## función de entrenamiento  y prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjU1Dq6DybzF"
      },
      "source": [
        "def fitting(model,train_x,train_y,test_x,test_y,EPOCHS,N_batch,batch_size):\n",
        "  for epoch in range(EPOCHS):\n",
        "    i=0\n",
        "    while i+batch_size < len(train_x) or i+batch_size<batch_size*N_batch:\n",
        "      start = i\n",
        "      end = i+batch_size\n",
        "      batch_x = train_x[start:end]\n",
        "      batch_y = train_y[start:end]\n",
        "      train_step(model,batch_x,batch_y)\n",
        "      i+=batch_size\n",
        "\n",
        "    test_step(model,test_x,test_y)\n",
        "    if epoch%50==0:  \n",
        "      template = 'Epoch {}, Perdida: {}, Perdida de prueba: {}'\n",
        "      print(template.format(epoch+1,train_loss.result(),test_loss.result()))\n",
        "    train_loss.reset_states()\n",
        "    test_loss.reset_states()\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEPeNWVwy1Q7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362f2e9b-d636-417d-eed8-f2b6487a40ff"
      },
      "source": [
        "#Debido a que x es igual a y, se tiene x_train dos veces\r\n",
        "#Para que funcione correctamente, la pérdida debe llegar a 8\r\n",
        "fitting(DNN,x_train,x_train,x_test,x_test,20000,3000,20) #190000 epochs para que funcione con 2 caracteristicas. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Perdida: 6975.1943359375, Perdida de prueba: 6790.48779296875\n",
            "Epoch 51, Perdida: 3638.0419921875, Perdida de prueba: 3634.70947265625\n",
            "Epoch 101, Perdida: 3236.8876953125, Perdida de prueba: 3233.31396484375\n",
            "Epoch 151, Perdida: 3030.46142578125, Perdida de prueba: 3028.08984375\n",
            "Epoch 201, Perdida: 2834.711181640625, Perdida de prueba: 2825.051513671875\n",
            "Epoch 251, Perdida: 2680.770263671875, Perdida de prueba: 2667.947265625\n",
            "Epoch 301, Perdida: 2565.832275390625, Perdida de prueba: 2551.448486328125\n",
            "Epoch 351, Perdida: 2462.308837890625, Perdida de prueba: 2448.31005859375\n",
            "Epoch 401, Perdida: 2378.491455078125, Perdida de prueba: 2363.357666015625\n",
            "Epoch 451, Perdida: 2313.527587890625, Perdida de prueba: 2298.763671875\n",
            "Epoch 501, Perdida: 2266.333740234375, Perdida de prueba: 2254.408935546875\n",
            "Epoch 551, Perdida: 2229.067138671875, Perdida de prueba: 2216.396728515625\n",
            "Epoch 601, Perdida: 2199.19384765625, Perdida de prueba: 2189.90869140625\n",
            "Epoch 651, Perdida: 2172.199462890625, Perdida de prueba: 2165.882568359375\n",
            "Epoch 701, Perdida: 2149.59765625, Perdida de prueba: 2145.25146484375\n",
            "Epoch 751, Perdida: 2129.950439453125, Perdida de prueba: 2126.58837890625\n",
            "Epoch 801, Perdida: 2110.1103515625, Perdida de prueba: 2108.833984375\n",
            "Epoch 851, Perdida: 2094.109375, Perdida de prueba: 2091.150146484375\n",
            "Epoch 901, Perdida: 2079.121826171875, Perdida de prueba: 2078.857421875\n",
            "Epoch 951, Perdida: 2065.294189453125, Perdida de prueba: 2065.77001953125\n",
            "Epoch 1001, Perdida: 2052.02001953125, Perdida de prueba: 2056.193359375\n",
            "Epoch 1051, Perdida: 2040.8887939453125, Perdida de prueba: 2044.04638671875\n",
            "Epoch 1101, Perdida: 2029.915771484375, Perdida de prueba: 2035.41748046875\n",
            "Epoch 1151, Perdida: 2018.503173828125, Perdida de prueba: 2026.44189453125\n",
            "Epoch 1201, Perdida: 2010.1180419921875, Perdida de prueba: 2019.628173828125\n",
            "Epoch 1251, Perdida: 2002.0787353515625, Perdida de prueba: 2010.590087890625\n",
            "Epoch 1301, Perdida: 1993.9903564453125, Perdida de prueba: 2006.2288818359375\n",
            "Epoch 1351, Perdida: 1986.076904296875, Perdida de prueba: 1997.2550048828125\n",
            "Epoch 1401, Perdida: 1979.8079833984375, Perdida de prueba: 1992.407470703125\n",
            "Epoch 1451, Perdida: 1973.8873291015625, Perdida de prueba: 1987.29443359375\n",
            "Epoch 1501, Perdida: 1967.579833984375, Perdida de prueba: 1980.3431396484375\n",
            "Epoch 1551, Perdida: 1961.064697265625, Perdida de prueba: 1975.238525390625\n",
            "Epoch 1601, Perdida: 1956.3582763671875, Perdida de prueba: 1970.557861328125\n",
            "Epoch 1651, Perdida: 1951.2838134765625, Perdida de prueba: 1966.439208984375\n",
            "Epoch 1701, Perdida: 1945.7451171875, Perdida de prueba: 1962.0985107421875\n",
            "Epoch 1751, Perdida: 1940.8782958984375, Perdida de prueba: 1957.107421875\n",
            "Epoch 1801, Perdida: 1936.3663330078125, Perdida de prueba: 1955.4146728515625\n",
            "Epoch 1851, Perdida: 1931.59033203125, Perdida de prueba: 1950.8814697265625\n",
            "Epoch 1901, Perdida: 1925.9361572265625, Perdida de prueba: 1947.801025390625\n",
            "Epoch 1951, Perdida: 1922.8463134765625, Perdida de prueba: 1945.569580078125\n",
            "Epoch 2001, Perdida: 1918.0653076171875, Perdida de prueba: 1940.4896240234375\n",
            "Epoch 2051, Perdida: 1915.374267578125, Perdida de prueba: 1936.23828125\n",
            "Epoch 2101, Perdida: 1911.0714111328125, Perdida de prueba: 1935.810302734375\n",
            "Epoch 2151, Perdida: 1908.1239013671875, Perdida de prueba: 1931.045166015625\n",
            "Epoch 2201, Perdida: 1905.177734375, Perdida de prueba: 1930.0269775390625\n",
            "Epoch 2251, Perdida: 1902.5565185546875, Perdida de prueba: 1928.5096435546875\n",
            "Epoch 2301, Perdida: 1898.6912841796875, Perdida de prueba: 1926.2838134765625\n",
            "Epoch 2351, Perdida: 1895.5433349609375, Perdida de prueba: 1923.4805908203125\n",
            "Epoch 2401, Perdida: 1893.0413818359375, Perdida de prueba: 1920.3277587890625\n",
            "Epoch 2451, Perdida: 1889.5260009765625, Perdida de prueba: 1919.3616943359375\n",
            "Epoch 2501, Perdida: 1887.2701416015625, Perdida de prueba: 1917.473388671875\n",
            "Epoch 2551, Perdida: 1884.36962890625, Perdida de prueba: 1916.251708984375\n",
            "Epoch 2601, Perdida: 1881.970947265625, Perdida de prueba: 1913.2611083984375\n",
            "Epoch 2651, Perdida: 1880.660888671875, Perdida de prueba: 1913.0316162109375\n",
            "Epoch 2701, Perdida: 1875.9249267578125, Perdida de prueba: 1911.84765625\n",
            "Epoch 2751, Perdida: 1874.5113525390625, Perdida de prueba: 1909.422607421875\n",
            "Epoch 2801, Perdida: 1872.497802734375, Perdida de prueba: 1907.880126953125\n",
            "Epoch 2851, Perdida: 1870.608642578125, Perdida de prueba: 1905.6041259765625\n",
            "Epoch 2901, Perdida: 1868.316162109375, Perdida de prueba: 1903.7049560546875\n",
            "Epoch 2951, Perdida: 1865.7642822265625, Perdida de prueba: 1902.29833984375\n",
            "Epoch 3001, Perdida: 1863.9171142578125, Perdida de prueba: 1902.028564453125\n",
            "Epoch 3051, Perdida: 1862.2774658203125, Perdida de prueba: 1900.0814208984375\n",
            "Epoch 3101, Perdida: 1860.3212890625, Perdida de prueba: 1898.7506103515625\n",
            "Epoch 3151, Perdida: 1859.1180419921875, Perdida de prueba: 1897.4173583984375\n",
            "Epoch 3201, Perdida: 1857.478515625, Perdida de prueba: 1897.6910400390625\n",
            "Epoch 3251, Perdida: 1856.5570068359375, Perdida de prueba: 1896.002685546875\n",
            "Epoch 3301, Perdida: 1853.74755859375, Perdida de prueba: 1894.1385498046875\n",
            "Epoch 3351, Perdida: 1851.77490234375, Perdida de prueba: 1891.8656005859375\n",
            "Epoch 3401, Perdida: 1850.3592529296875, Perdida de prueba: 1892.0406494140625\n",
            "Epoch 3451, Perdida: 1848.4384765625, Perdida de prueba: 1890.17431640625\n",
            "Epoch 3501, Perdida: 1847.4696044921875, Perdida de prueba: 1888.8011474609375\n",
            "Epoch 3551, Perdida: 1846.5570068359375, Perdida de prueba: 1888.4166259765625\n",
            "Epoch 3601, Perdida: 1843.61181640625, Perdida de prueba: 1887.0721435546875\n",
            "Epoch 3651, Perdida: 1843.5916748046875, Perdida de prueba: 1885.9793701171875\n",
            "Epoch 3701, Perdida: 1841.5062255859375, Perdida de prueba: 1885.2359619140625\n",
            "Epoch 3751, Perdida: 1838.9581298828125, Perdida de prueba: 1883.921630859375\n",
            "Epoch 3801, Perdida: 1837.681396484375, Perdida de prueba: 1883.5048828125\n",
            "Epoch 3851, Perdida: 1836.3013916015625, Perdida de prueba: 1881.9388427734375\n",
            "Epoch 3901, Perdida: 1836.3851318359375, Perdida de prueba: 1882.31396484375\n",
            "Epoch 3951, Perdida: 1834.291259765625, Perdida de prueba: 1880.572265625\n",
            "Epoch 4001, Perdida: 1833.6038818359375, Perdida de prueba: 1880.711669921875\n",
            "Epoch 4051, Perdida: 1832.0638427734375, Perdida de prueba: 1879.0880126953125\n",
            "Epoch 4101, Perdida: 1830.5372314453125, Perdida de prueba: 1877.8692626953125\n",
            "Epoch 4151, Perdida: 1830.322265625, Perdida de prueba: 1878.036376953125\n",
            "Epoch 4201, Perdida: 1829.76953125, Perdida de prueba: 1877.286376953125\n",
            "Epoch 4251, Perdida: 1828.269775390625, Perdida de prueba: 1877.65576171875\n",
            "Epoch 4301, Perdida: 1827.186279296875, Perdida de prueba: 1876.580810546875\n",
            "Epoch 4351, Perdida: 1825.7093505859375, Perdida de prueba: 1876.4893798828125\n",
            "Epoch 4401, Perdida: 1825.797607421875, Perdida de prueba: 1874.9532470703125\n",
            "Epoch 4451, Perdida: 1824.2025146484375, Perdida de prueba: 1874.4886474609375\n",
            "Epoch 4501, Perdida: 1822.0723876953125, Perdida de prueba: 1873.882080078125\n",
            "Epoch 4551, Perdida: 1821.5364990234375, Perdida de prueba: 1872.9940185546875\n",
            "Epoch 4601, Perdida: 1820.0303955078125, Perdida de prueba: 1872.4832763671875\n",
            "Epoch 4651, Perdida: 1819.7093505859375, Perdida de prueba: 1871.7408447265625\n",
            "Epoch 4701, Perdida: 1818.8914794921875, Perdida de prueba: 1871.3670654296875\n",
            "Epoch 4751, Perdida: 1818.0587158203125, Perdida de prueba: 1870.0848388671875\n",
            "Epoch 4801, Perdida: 1817.08349609375, Perdida de prueba: 1871.17578125\n",
            "Epoch 4851, Perdida: 1816.524658203125, Perdida de prueba: 1870.3934326171875\n",
            "Epoch 4901, Perdida: 1815.804931640625, Perdida de prueba: 1869.3397216796875\n",
            "Epoch 4951, Perdida: 1814.87109375, Perdida de prueba: 1869.6199951171875\n",
            "Epoch 5001, Perdida: 1813.8353271484375, Perdida de prueba: 1869.1573486328125\n",
            "Epoch 5051, Perdida: 1812.7900390625, Perdida de prueba: 1868.9691162109375\n",
            "Epoch 5101, Perdida: 1811.4080810546875, Perdida de prueba: 1867.6484375\n",
            "Epoch 5151, Perdida: 1811.571533203125, Perdida de prueba: 1866.9881591796875\n",
            "Epoch 5201, Perdida: 1810.963623046875, Perdida de prueba: 1866.1121826171875\n",
            "Epoch 5251, Perdida: 1810.0174560546875, Perdida de prueba: 1867.1171875\n",
            "Epoch 5301, Perdida: 1808.9278564453125, Perdida de prueba: 1864.872314453125\n",
            "Epoch 5351, Perdida: 1808.138427734375, Perdida de prueba: 1866.08642578125\n",
            "Epoch 5401, Perdida: 1807.6368408203125, Perdida de prueba: 1864.426025390625\n",
            "Epoch 5451, Perdida: 1807.0361328125, Perdida de prueba: 1865.7530517578125\n",
            "Epoch 5501, Perdida: 1806.618408203125, Perdida de prueba: 1864.8311767578125\n",
            "Epoch 5551, Perdida: 1805.599853515625, Perdida de prueba: 1865.4798583984375\n",
            "Epoch 5601, Perdida: 1805.68701171875, Perdida de prueba: 1864.79931640625\n",
            "Epoch 5651, Perdida: 1804.5919189453125, Perdida de prueba: 1863.1234130859375\n",
            "Epoch 5701, Perdida: 1803.259765625, Perdida de prueba: 1862.669921875\n",
            "Epoch 5751, Perdida: 1803.0360107421875, Perdida de prueba: 1862.7452392578125\n",
            "Epoch 5801, Perdida: 1802.1572265625, Perdida de prueba: 1862.544189453125\n",
            "Epoch 5851, Perdida: 1801.8988037109375, Perdida de prueba: 1862.7471923828125\n",
            "Epoch 5901, Perdida: 1801.0916748046875, Perdida de prueba: 1861.755859375\n",
            "Epoch 5951, Perdida: 1800.74609375, Perdida de prueba: 1861.745361328125\n",
            "Epoch 6001, Perdida: 1799.9466552734375, Perdida de prueba: 1860.9749755859375\n",
            "Epoch 6051, Perdida: 1799.1826171875, Perdida de prueba: 1861.4205322265625\n",
            "Epoch 6101, Perdida: 1798.503173828125, Perdida de prueba: 1861.70751953125\n",
            "Epoch 6151, Perdida: 1798.681396484375, Perdida de prueba: 1859.728759765625\n",
            "Epoch 6201, Perdida: 1798.45849609375, Perdida de prueba: 1859.44140625\n",
            "Epoch 6251, Perdida: 1797.080322265625, Perdida de prueba: 1859.830810546875\n",
            "Epoch 6301, Perdida: 1796.4981689453125, Perdida de prueba: 1859.09765625\n",
            "Epoch 6351, Perdida: 1796.326904296875, Perdida de prueba: 1858.4892578125\n",
            "Epoch 6401, Perdida: 1796.1578369140625, Perdida de prueba: 1859.27001953125\n",
            "Epoch 6451, Perdida: 1795.5106201171875, Perdida de prueba: 1858.20263671875\n",
            "Epoch 6501, Perdida: 1794.7947998046875, Perdida de prueba: 1857.3719482421875\n",
            "Epoch 6551, Perdida: 1793.986328125, Perdida de prueba: 1858.3831787109375\n",
            "Epoch 6601, Perdida: 1793.586669921875, Perdida de prueba: 1856.8817138671875\n",
            "Epoch 6651, Perdida: 1792.7435302734375, Perdida de prueba: 1857.387939453125\n",
            "Epoch 6701, Perdida: 1793.4925537109375, Perdida de prueba: 1856.5980224609375\n",
            "Epoch 6751, Perdida: 1792.888916015625, Perdida de prueba: 1855.8182373046875\n",
            "Epoch 6801, Perdida: 1792.3114013671875, Perdida de prueba: 1856.096923828125\n",
            "Epoch 6851, Perdida: 1791.7626953125, Perdida de prueba: 1856.2606201171875\n",
            "Epoch 6901, Perdida: 1791.4228515625, Perdida de prueba: 1855.189453125\n",
            "Epoch 6951, Perdida: 1790.59619140625, Perdida de prueba: 1855.0146484375\n",
            "Epoch 7001, Perdida: 1790.450927734375, Perdida de prueba: 1854.581787109375\n",
            "Epoch 7051, Perdida: 1790.216796875, Perdida de prueba: 1854.1240234375\n",
            "Epoch 7101, Perdida: 1789.9444580078125, Perdida de prueba: 1854.573974609375\n",
            "Epoch 7151, Perdida: 1788.55615234375, Perdida de prueba: 1854.0546875\n",
            "Epoch 7201, Perdida: 1788.5003662109375, Perdida de prueba: 1854.0950927734375\n",
            "Epoch 7251, Perdida: 1788.47509765625, Perdida de prueba: 1854.12646484375\n",
            "Epoch 7301, Perdida: 1787.683349609375, Perdida de prueba: 1854.3104248046875\n",
            "Epoch 7351, Perdida: 1788.173095703125, Perdida de prueba: 1853.5833740234375\n",
            "Epoch 7401, Perdida: 1787.043212890625, Perdida de prueba: 1853.1939697265625\n",
            "Epoch 7451, Perdida: 1786.8779296875, Perdida de prueba: 1853.807861328125\n",
            "Epoch 7501, Perdida: 1787.0692138671875, Perdida de prueba: 1853.7103271484375\n",
            "Epoch 7551, Perdida: 1785.357421875, Perdida de prueba: 1852.9957275390625\n",
            "Epoch 7601, Perdida: 1785.3673095703125, Perdida de prueba: 1851.8323974609375\n",
            "Epoch 7651, Perdida: 1785.2086181640625, Perdida de prueba: 1852.218994140625\n",
            "Epoch 7701, Perdida: 1785.0372314453125, Perdida de prueba: 1851.419921875\n",
            "Epoch 7751, Perdida: 1784.309326171875, Perdida de prueba: 1851.6683349609375\n",
            "Epoch 7801, Perdida: 1784.0208740234375, Perdida de prueba: 1850.9327392578125\n",
            "Epoch 7851, Perdida: 1785.0291748046875, Perdida de prueba: 1851.606201171875\n",
            "Epoch 7901, Perdida: 1783.242431640625, Perdida de prueba: 1850.251220703125\n",
            "Epoch 7951, Perdida: 1782.640380859375, Perdida de prueba: 1850.7755126953125\n",
            "Epoch 8001, Perdida: 1782.8968505859375, Perdida de prueba: 1850.56005859375\n",
            "Epoch 8051, Perdida: 1782.562744140625, Perdida de prueba: 1850.5399169921875\n",
            "Epoch 8101, Perdida: 1782.371826171875, Perdida de prueba: 1851.4356689453125\n",
            "Epoch 8151, Perdida: 1782.216064453125, Perdida de prueba: 1850.6641845703125\n",
            "Epoch 8201, Perdida: 1781.3037109375, Perdida de prueba: 1848.839111328125\n",
            "Epoch 8251, Perdida: 1781.54736328125, Perdida de prueba: 1850.091064453125\n",
            "Epoch 8301, Perdida: 1780.6068115234375, Perdida de prueba: 1849.634521484375\n",
            "Epoch 8351, Perdida: 1780.1707763671875, Perdida de prueba: 1850.050048828125\n",
            "Epoch 8401, Perdida: 1779.9794921875, Perdida de prueba: 1848.416259765625\n",
            "Epoch 8451, Perdida: 1780.261474609375, Perdida de prueba: 1849.428466796875\n",
            "Epoch 8501, Perdida: 1779.89501953125, Perdida de prueba: 1849.2811279296875\n",
            "Epoch 8551, Perdida: 1780.409912109375, Perdida de prueba: 1849.0211181640625\n",
            "Epoch 8601, Perdida: 1777.970947265625, Perdida de prueba: 1849.00439453125\n",
            "Epoch 8651, Perdida: 1778.56005859375, Perdida de prueba: 1847.873046875\n",
            "Epoch 8701, Perdida: 1778.2589111328125, Perdida de prueba: 1848.7891845703125\n",
            "Epoch 8751, Perdida: 1778.2403564453125, Perdida de prueba: 1848.142578125\n",
            "Epoch 8801, Perdida: 1777.72021484375, Perdida de prueba: 1847.409423828125\n",
            "Epoch 8851, Perdida: 1778.1790771484375, Perdida de prueba: 1847.791748046875\n",
            "Epoch 8901, Perdida: 1776.8326416015625, Perdida de prueba: 1847.3798828125\n",
            "Epoch 8951, Perdida: 1776.9666748046875, Perdida de prueba: 1847.3594970703125\n",
            "Epoch 9001, Perdida: 1776.059326171875, Perdida de prueba: 1847.6783447265625\n",
            "Epoch 9051, Perdida: 1775.8408203125, Perdida de prueba: 1846.517822265625\n",
            "Epoch 9101, Perdida: 1776.56005859375, Perdida de prueba: 1847.5216064453125\n",
            "Epoch 9151, Perdida: 1776.3907470703125, Perdida de prueba: 1846.8450927734375\n",
            "Epoch 9201, Perdida: 1775.99072265625, Perdida de prueba: 1847.043701171875\n",
            "Epoch 9251, Perdida: 1775.3172607421875, Perdida de prueba: 1846.3902587890625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpb_hRYc_b3v"
      },
      "source": [
        "plt.imshow(np.reshape(x_train[0],(28,28)),cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLzFJeh41Jbe"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "img=DNN(x_train[24:30])\r\n",
        "print(len(img[1]))\r\n",
        "\r\n",
        "plt.imshow(np.reshape(img[2],(28,28)),cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}